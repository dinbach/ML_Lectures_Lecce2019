{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Example: Classification of Signal and Background of a search for heavy resonance\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1. Download the Data\n",
    "For illustration we will use data from the $X \\rightarrow ZV \\rightarrow \\ell\\ell qq$ (V = W,Z) in the heavy Higgs scenario production through gluon-gluon fusion (ggF). We'll use backgrounds of Top, Diboson and Z+jets events.\n",
    "\n",
    "<img src=\"Feynman-all.png\" width=\"600\">\n",
    "\n",
    "Root files of signal and background processes you can find here: [LLqqFiles](https://drive.google.com/open?id=1MPzf01CJx8kbix0ko30SbYO7xlUd4u1W)\n",
    "\n",
    "Download the zip file and put them all under a `Files/` directory. The zip file contains:\n",
    "\n",
    "```python\n",
    "Files/Top.root\n",
    "Files/Diboson.root\n",
    "Files/Zjets.root\n",
    "Files/ggH1000.root # signal of heavy Higgs with M=1TeV\n",
    "```\n",
    "\n",
    "Each root file contains a TTree with name `Nominal` filled with a set of variables filled during offline analysis. Some preselection has already been done. Each event here contains exactly 2 leptons and 1 FatJet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries to use later\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](logo-uproot.png)\n",
    "\n",
    "We will read the root file and covert it to a Pandas DataFrame using **uproot**. Uproot is a software package for I/O methods between ROOT and numpy and pandas. There is a huge number of functionalities for ROOT and Pandas file manipulation. Check-out this link: [Uproot page](https://github.com/scikit-hep/uproot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets have a look at the number of events of each file\n",
    "\n",
    "inputFiles = ['Top','Zjets','Diboson','ggH1000']\n",
    "\n",
    "for i in inputFiles:\n",
    "    inFile = 'Files/'+i+'.root'\n",
    "    theFile = uproot.open(inFile)\n",
    "    tree = uproot.open(inFile)['Nominal']\n",
    "    Nevents = uproot.open(inFile)['Nominal'].numentries\n",
    "    print ('Number of events in '+inFile,'\\t'+str(Nevents))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all of them to Pandas Dataframes\n",
    "DF_Signal  = uproot.open('Files/ggH1000.root')['Nominal'].pandas.df()\n",
    "DF_Top     = uproot.open('Files/Top.root')['Nominal'].pandas.df()\n",
    "DF_Zjets   = uproot.open('Files/Zjets.root')['Nominal'].pandas.df()\n",
    "DF_Diboson = uproot.open('Files/Diboson.root')['Nominal'].pandas.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)\n",
    "DF_Top.head()\n",
    "# print(DF_Top.head())\n",
    "# print(DF_Zjets.head())\n",
    "# print(DF_Diboson.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets print few events to see the content\n",
    "DF_Signal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "## 3. Variable Description\n",
    "\n",
    "Basic info from both leptons and the FatJet. ( ùëùùëá , ùúÇ , ùúô , ùê∏ , lepton charge)\n",
    "\n",
    "Dilepton ùëùùëá  and mass\n",
    "\n",
    "More advanced features for the FatJet. Jet substructure variables  ùê∑2,ùê∂2 \n",
    "\n",
    "**FullEventWeight** is the generator MC weight * all other Scale Factor weights used in the llqq analysis\n",
    "\n",
    "**isSignal** in an ``int`` which declares whether the event is signal(=1) or backround (=0). We'll use this variable later on as label for each event in the DNN training and testing process. \n",
    "\n",
    "**Note**: if this kind of variable is missing from your initial .root file then it's still possible to add it with DF or NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 4. Visualizing input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of inputs and results can be quickly done with [**Matplotlib**](https://matplotlib.org/)\n",
    "\n",
    "Lets try to plot the variables in the DFs for signal and background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "VariablesToPlot = ['lep1_pt','lep2_pt','lep1_E','lep2_E','Zll_mass','Zll_pt','MET','fatjet_pt']\n",
    "\n",
    "for var in VariablesToPlot:\n",
    "    #adopt a common binning scheme for all channels\n",
    "    bins = np.linspace(min(DF_Top[var]), max(DF_Top[var]) + 1, 100)\n",
    "    \n",
    "    plt.hist(DF_Top[var], histtype='step', normed=True, bins=bins, label='Top', linewidth=2)\n",
    "    plt.hist(DF_Signal[var], histtype='step', normed=True, bins=bins, label='ggFH', linewidth=2)\n",
    "    plt.hist(DF_Diboson[var], histtype='step', normed=True, bins=bins, label='Diboson', linewidth=2)\n",
    "    plt.hist(DF_Zjets[var], histtype='step', normed=True, bins=bins, label='Z+jets', linewidth=2)\n",
    "    \n",
    "    plt.xlabel(var)\n",
    "#     plt.yscale('log')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### 5. Preparing your data as inputs to a DNN\n",
    "\n",
    "We have already converted the .root files for signal and background to Pandas DFs. You can then use these for the rest of the lecture or you can save to disk in DataFrame or other format if you want to start from were you've left off.\n",
    "\n",
    "Saving to disk you can do with the `to_pickle(filename.pkl)` command like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving DF to disk\n",
    "DF_Top.to_pickle('Files/Top_DF.pkl')\n",
    "DF_Diboson.to_pickle('Files/Diboson_DF.pkl')\n",
    "DF_Zjets.to_pickle('Files/Zjets_DF.pkl')\n",
    "DF_Signal.to_pickle('Files/ggH1000_DF.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then read back your .pkl files with commands like:\n",
    "\n",
    "```python\n",
    "df_tmp = pd.read_pickle('Files/Diboson_DF.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### 6. Prepare signal and background inputs and shuffle your data\n",
    "\n",
    "For simplicity we are going to have two main classes to ask the DNN to make predictions. One Signal and one Background class. If you wish, you can make this a multi-classification problem by introducing a separate class for each background source but for the moment we are keeping this as simple as possible.\n",
    "\n",
    "Lets add all background sources together in one DF file. This you can do with the `pd.concat` command as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.utils\n",
    "def Shuffling(df):\n",
    "    df = sklearn.utils.shuffle(df,random_state=123) #'123' is the random seed\n",
    "    df = df.reset_index(drop=True)# drop=True does not allow the reset_index to insert a new column with the suffled index\n",
    "    return df\n",
    "\n",
    "# Now lets shuffle the events in each file\n",
    "DF_Signal = Shuffling(DF_Signal)\n",
    "DF_Zjets  = Shuffling(DF_Zjets)\n",
    "DF_Top    = Shuffling(DF_Top)\n",
    "DF_Diboson= Shuffling(DF_Diboson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the files in lists\n",
    "signalList = [DF_Signal[:50000] ] #just 1 file in this list but depending on your input you might want to extend this.\n",
    "backgroundList = [DF_Top[:17000],DF_Diboson[:17000],DF_Zjets[:17000]] # 3 files in the background list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many files are there\n",
    "len(backgroundList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets add them together\n",
    "totalPD_sig = pd.concat(signalList,ignore_index=True)\n",
    "totalPD_bkg = pd.concat(backgroundList,ignore_index=True) #add all channels of background together to get 1 DF\n",
    "\n",
    "#print the number of event in each\n",
    "print ('Signal events:',totalPD_sig.shape)\n",
    "print ('Background events:',totalPD_bkg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add together signal+background PDs\n",
    "df_total = pd.concat([totalPD_sig,totalPD_bkg],ignore_index=True)\n",
    "\n",
    "# randomize signal+background  events\n",
    "df_total = Shuffling(df_total)\n",
    "print (\"Total events after shuffling: \",df_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savePD\n",
    "df_total.to_pickle('Files/MixData_PD.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 7. Building and Training a Deep Neural Network with Keras\n",
    "\n",
    "### Model Representation and input arrays\n",
    "- __X__ : Denotes the \"input\" variables. Is a Numpy array of dimensions [events, variables] containing the distributions to be used as inputs to the DNN.\n",
    "\n",
    "- **y** : Denotes the \"output\" or target variable that we are trying to predict. Is an array of dimension (events). Since here we want to tell  S from B (binary classification problem) y can take  only two values, 0 and 1 indicating Background or Signal respectively. (Most of what we say here will also generalize to the multiple-class case.) y is also called the label for the training example.\n",
    "\n",
    "Our goal is, given a training set, to learn a function **h : X $\\rightarrow$ Y** so that **h(x)** is a \"good\" predictor for the corresponding value of **y.**\n",
    "\n",
    "<br>\n",
    "Lets create the arrays described above. We are going to use only a subset of the available variables for X but you can excercise with different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputFeatures = ['lep1_pt','lep1_eta','lep1_phi','lep1_E', 'lep2_pt','lep2_eta','lep2_phi','lep2_E', 'Zll_mass','Zll_pt', 'MET']\n",
    "# make the X array\n",
    "X=df_total[InputFeatures].values\n",
    "\n",
    "#make y vector\n",
    "y_tmp = df_total['isSignal']\n",
    "\n",
    "# make the event weights vector\n",
    "w=df_total['FullEventWeight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets print the arrays before scaling\n",
    "print(X)\n",
    "print(y_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Feature Scaling\n",
    "We can speed up gradient descent by having each of our input values in roughly the same range. This is because minimization of the cost function with respect to the parameters will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
    "\n",
    "The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. This can be done using the **StandardScaler** module from the **scikit-learn** (http://scikit-learn.org) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets print the arrays after scaling\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Create train and test samples\n",
    "We need to split our dataset in 3 parts in order to evaluate its performance and be able to tell how well our model generarizes to unseen data. Split dataset into multiple parts:\n",
    "- **Training set**: Used to fit model parameters\n",
    "- **Validation set**: Used to check performance on independent data and tune hyper parameters\n",
    "- **Test set**: Final evaluation of performance after all hyper-parameters are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split into training and testing samples\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(X, y, w, train_size=0.7,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Building a DNN in Keras\n",
    "\n",
    "Models in Keras are defined as a sequence of layers. We create a Sequential model and add 'hidden' layers.\n",
    "\n",
    "In this example, we will use a fully-connected network structure with 3 layers.\n",
    "\n",
    "Fully connected layers are defined using the **Dense** class. We can specify the number of neurons in the layer with  `units=width` and specify the activation function using the `Activation('relu')` argument. We need to make sure that the input layer has the right number of inputs. This can be specified when creating the first layer with the `input_dim` argument and setting it to be equal to the shape of the X_train dataset `n_dim=X_train.shape[1]`.\n",
    "\n",
    "```python\n",
    "n_dim=X_train.shape[1]\n",
    "n_nodes = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=n_nodes, input_dim=n_dim))\n",
    "model.add(Activation('relu'))\n",
    "```\n",
    "\n",
    "Network weights are initialized from a small random number generated from a uniform distribution (‚Äòuniform‚Äò), in this case between 0 and 0.05 because that is the default uniform weight initialization in Keras. \n",
    "\n",
    "We add more layers and use the rectifier (‚Äòrelu‚Äò) activation function for them.\n",
    "\n",
    "```python\n",
    "n_depth = 2 \n",
    "for i in range(0, depth):\n",
    "        model.add(Dense(width))\n",
    "        model.add(Activation('relu'))\n",
    "```\n",
    "\n",
    "We use a sigmoid on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class\n",
    "\n",
    "```python\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "<br>\n",
    "Lets put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildDNN(N_input,width,depth):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=width, input_dim=N_input) )  # First layer\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    for i in range(0, depth):\n",
    "        model.add(Dense(width))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer/node \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now lets construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim=X_train.shape[1]\n",
    "n_nodes = 32\n",
    "n_depth = 2 # number of additional hidden layers\n",
    "\n",
    "model=BuildDNN(n_dim,n_nodes,n_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model uses the efficient numerical computation libraries (backends) such as **Theano** or **TensorFlow**.\n",
    "\n",
    "When compiling, we must specify the loss function to use to evaluate a set of weights, the optimizer used to search through different weights for the network and any optional metrics we would like to collect and report during training.\n",
    "\n",
    "In this case, we will use logarithmic loss, which for a binary classification problem is defined in Keras as `'binary_crossentropy'`. We will also use the gradient descent algorithm `rmsprop`. `adam` is another popular option.\n",
    "\n",
    "Finally we use the classification accuracy as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train or fit the model on the loaded data by calling the **fit()** function.\n",
    "\n",
    "The training process will run for a fixed number of iterations through the dataset called **epochs**. We can also set the number of instances that are evaluated before a weight update in the network is performed, called the batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training!\n",
    "modelMetricsHistory = model.fit(X_train, y_train, epochs=20,batch_size=2048,validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = model.evaluate(X_test, y_test, batch_size=2048)\n",
    "testLoss = 'Test loss:%0.3f' % perf[0]\n",
    "testAccuracy = 'Test accuracy:%0.3f' % perf[1]\n",
    "print (testLoss)\n",
    "print (testAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(modelMetricsHistory.history['acc'])\n",
    "plt.plot(modelMetricsHistory.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.figtext(0.5, 0.5, testAccuracy, wrap=True, horizontalalignment='center', fontsize=10)\n",
    "plt.savefig(\"Files/Accuracy.png\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "# summarize history for loss\n",
    "plt.plot(modelMetricsHistory.history['loss'])\n",
    "plt.plot(modelMetricsHistory.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.figtext(0.5, 0.5, testLoss, wrap=True, horizontalalignment='center', fontsize=10)\n",
    "plt.savefig(\"Files/Loss.png\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Using regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildDNN_dropout(N_input,width,depth):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=width, input_dim=N_input, kernel_constraint=maxnorm(3) ) )  # First layer\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    for i in range(0, depth):\n",
    "        model.add(Dense(width))\n",
    "        model.add(Activation('relu'))\n",
    "        # Dropout randomly sets a fraction of input units to 0 at each update during training time\n",
    "        # which helps prevent overfitting.\n",
    "        model.add(Dropout(0.2)) \n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer/node \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dro=BuildDNN_dropout(n_dim,n_nodes,n_depth)\n",
    "model_dro.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "# Start the training!\n",
    "history_dro = model_dro.fit(X_train, y_train, epochs=20,batch_size=2048,validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history_dro.history['acc'])\n",
    "plt.plot(history_dro.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='lower right')\n",
    "plt.figtext(0.5, 0.5, testAccuracy, wrap=True, horizontalalignment='center', fontsize=10)\n",
    "plt.savefig(\"Files/Accuracy.png\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "# summarize history for loss\n",
    "plt.plot(history_dro.history['loss'])\n",
    "plt.plot(history_dro.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.figtext(0.5, 0.5, testLoss, wrap=True, horizontalalignment='center', fontsize=10)\n",
    "plt.savefig(\"Files/Loss.png\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Adding callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "# if we don't have an increase of the accuracy for 10 epochs, terminate training.\n",
    "EarlyStopping(verbose=True, patience=10, monitor='val_loss'),\n",
    "# Always make sure that we're saving the model weights with the best loss.\n",
    "ModelCheckpoint('model.h5', monitor='val_loss', verbose=True, save_best_only=True)\n",
    "]\n",
    "\n",
    "model_dro =BuildDNN_dropout(n_dim,n_nodes,n_depth)\n",
    "model_dro.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# Start the training!\n",
    "history_dro = model_dro.fit(X_train, y_train, callbacks=callbacks, epochs=200,batch_size=2048,validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you have the best model saved in your directory as 'model.h5'. You can then load it back and use it !**\n",
    "\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history_dro.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history_dro.history['acc'])\n",
    "plt.plot(history_dro.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history_dro.history['loss'])\n",
    "plt.plot(history_dro.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### User-defined metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "model_dro =BuildDNN_dropout(n_dim,n_nodes,n_depth)\n",
    "\n",
    "model_dro.compile(loss='binary_crossentropy', optimizer='adam', metrics=[rmse])\n",
    "# Start the training!\n",
    "history_dro = model_dro.fit(X_train, y_train, epochs=20,batch_size=2048,validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot metrics\n",
    "plt.plot(history_dro.history['rmse'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 8. DNN results\n",
    "Once the training process has been completed we can ask the trained DNN to make predictions on unseen data and in this way evaluate its performance. We therefore apply the model to the **test** sample we had created before.\n",
    "\n",
    "We use:\n",
    "```python\n",
    "model.predict(test_sample)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report\n",
    "\n",
    "# Generates output predictions for the input samples.\n",
    "yhat_test = model_dro.predict(X_test,batch_size=2048)\n",
    "\n",
    "print (yhat_test)\n",
    "\n",
    "scoreBins = np.linspace(0,1.0, 30)\n",
    "plt.hist(yhat_test,bins=scoreBins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to say how well the model is predicting the correct result is the ROC curve. A ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. \n",
    "\n",
    "TPR is defined as the correctly identified signal over the sum of correctly identified signal +¬†incorrectly rejected signal\n",
    "```text\n",
    "TPR =¬†N_passing_s/(N_passing_s+¬†N_rejected_s) ¬†or written as TPR = TP/(TP+FN)\n",
    "```\n",
    "and\n",
    "```text\n",
    "FPR =¬†N_passing_b/(N_passing_b+ N_rejected_b) or FPR = FP/(FP+TN)\n",
    "```\n",
    "\n",
    "so in this case TPR is equivalent to¬†Signal Efficiency and FPR is equivalent to Background Efficiency\n",
    "\n",
    "The **Scikit-learn** library offers the functionality of easily getting a ROC curve. Just plug-in `y_test` and the prediction `yhat_test`.\n",
    "\n",
    "The Area Under Curve (AUC) is another metric for the DNN performance and is actually the integral of the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 'Receiver operating characteristic' (ROC)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, yhat_test)\n",
    "\n",
    "# Compute Area Under the Curve (AUC) from prediction scores\n",
    "roc_auc  = auc(fpr, tpr)\n",
    "print (\"ROC AUC: %0.3f\" % roc_auc)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange',  lw=2, label='Full curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 0], [1, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC curves for Signal vs Background')\n",
    "# plt.plot([0.038], [0.45], marker='*', color='red',markersize=5, label=\"Cut-based\",linestyle=\"None\")\n",
    "# plt.plot([0.038, 0.038], [0,1], color='red', lw=1, linestyle='--') # same background rejection point\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.savefig(\"Files/ROC.pdf\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test, w_train, w_test\n",
    "\n",
    "Xtrain_signal     = X_train[y_train==1]\n",
    "Xtrain_background = X_train[y_train!=1]\n",
    "\n",
    "# Then do the same for Xtest\n",
    "Xtest_signal     = X_test[y_test==1]\n",
    "Xtest_background = X_test[y_test!=1]\n",
    "\n",
    "# Get predictions of the model on these -train- samples\n",
    "print ('Running model prediction on Xtrain_signal')\n",
    "yhat_train_signal     = model_dro.predict(Xtrain_signal,batch_size=2048)\n",
    "print ('Running model prediction on Xtrain_background')\n",
    "yhat_train_background = model_dro.predict(Xtrain_background,batch_size=2048)\n",
    "\n",
    "# Get predictions of the model on these -test- samples\n",
    "print ('Running model prediction on Xtest_signal')\n",
    "yhat_test_signal     = model_dro.predict(Xtest_signal,batch_size=2048)\n",
    "print ('Running model prediction on Xtest_background')\n",
    "yhat_test_background = model_dro.predict(Xtest_background,batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scores\n",
    "bins=np.linspace(0.0,1.0, 40)\n",
    "plt.hist(yhat_train_signal,     bins=bins, histtype='step',       lw=2, color='blue', label=[r'Signal Train'],     normed=True)\n",
    "plt.hist(yhat_test_signal,      bins=bins, histtype='stepfilled', lw=2, color='cyan', alpha=0.5,  label=[r'Signal Test'],      normed=True)\n",
    "\n",
    "plt.hist(yhat_train_background, bins=bins, histtype='step',       lw=2, color='red', label=[r'Background Train'], normed=True)\n",
    "plt.hist(yhat_test_background,  bins=bins, histtype='stepfilled', lw=2, color='orange', alpha=0.5,  label=[r'Background Test'],  normed=True)\n",
    "\n",
    "plt.ylabel('Norm. Entries')\n",
    "plt.xlabel('DNN score')\n",
    "# plt.yscale('log')\n",
    "# plt.ylim((0,1000))\n",
    "plt.legend(loc=\"upper center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "import itertools\n",
    "\n",
    "yResult_test_cls  = np.array([ int(round(x[0])) for x in yhat_test])\n",
    "theClasses = ['Background','Signal']\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes, rotation=90)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "cnf_matrix = confusion_matrix(y_test, yResult_test_cls)\n",
    "np.set_printoptions(precision=2)\n",
    "plot_confusion_matrix(cnf_matrix, classes=theClasses,normalize=True, title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputFeatures = ['lep1_pt','lep1_eta','lep1_phi','lep1_E', 'lep2_pt','lep2_eta','lep2_phi','lep2_E', 'Zll_mass','Zll_pt', 'MET']\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections  import Counter\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "df_tot = df_total\n",
    "\n",
    "X=df_tot[InputFeatures]\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "Y= df_tot['isSignal']\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=4, shuffle=False, random_state=None)\n",
    "cv_scores = []\n",
    "ii = 1\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1.0, 100)\n",
    "    \n",
    "for train_index, test_index in kfold.split(X, Y):\n",
    "    print('{} of KFold {}'.format(ii,kfold.n_splits))\n",
    "    \n",
    "    X_train_xval = np.array(X)[train_index,:]\n",
    "    X_test_xval = np.array(X)[test_index,:]\n",
    "    y_train_xval = np.array(Y)[train_index]\n",
    "    y_test_xval = np.array(Y)[test_index]\n",
    "    \n",
    "    print (X_train_xval.shape[1])\n",
    "\n",
    "    model = BuildDNN_dropout(X_train_xval.shape[1],32,3)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    # adjust the learning rate\n",
    "    K.set_value(model.optimizer.lr, 0.003 )\n",
    "    \n",
    "    callbacks = [\n",
    "            EarlyStopping(verbose=True, patience=5, monitor='val_loss'),\n",
    "            ModelCheckpoint('model_kfold_'+str(ii)+'.h5', monitor='val_loss', verbose=True, save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    kf_history = model.fit( X_train_xval, y_train_xval, epochs=200, batch_size=2048,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1)\n",
    "    \n",
    "    kf_y_prediction_test = model.predict(X_test_xval)\n",
    "    score = roc_auc_score(y_test_xval,kf_y_prediction_test)\n",
    "    print('ROC AUC score:',score)\n",
    "    cv_scores.append(score)    \n",
    "    \n",
    "    # Get 'Receiver operating characteristic' (ROC)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_xval, kf_y_prediction_test)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "#     print(roc_auc)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.3f)' % (ii, roc_auc))\n",
    "\n",
    "    ii += 1\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Luck', alpha=.8)\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',label=r'Mean ROC (AUC = %0.3f $\\pm$ %0.3f)' % (mean_auc, std_auc),lw=1, alpha=.7)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic example')\n",
    "plt.xticks(np.arange(0.0, 1.1, 0.1))\n",
    "plt.yticks(np.arange(0.0, 1.1, 0.1))\n",
    "plt.title('ROC curves for Signal vs Background')\n",
    "plt.legend(loc=\"lower right\",fontsize='x-small')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
